library(funModeling)
data.body<-read.table(file="data_multiple_regression_exercice.csv")
data-body=data.frame(data.body)
data.body=data.frame(data.body)
data.body=data.frame(data.body)
attach(data.body)
data.body<-read.table(file="data_multiple_regression_exercice.csv")
data.body=data.frame(data.body)
attach(data.body)
#a)Dividir el dataset 80% para entrenamiento, 20% para prueba.
library(funModeling)
index_sample=get_sample(data = data.body, percentage_tr_rows=0.8, seed = 1234)
data_training = data.body[index_sample,]
data_test = data.body[-index_sample,]
#b)
#Se intenta explicar el peso corporal en función del resto de las variables
#Comenzamos con un análisis de las variables
boxplot(data_training[,], col = 'violet', main = "Conjunto de Entrenamiento")
install.packages('ggcorrplot')
library(ggcorrplot)
ggcorrplot(cor(data_training[,1:24]))
#Continuamos con una regresión de peso vs todas las variables:
lm1=lm(data_training$weight~., data = data_training)
summary(lm1)
#se observa alta significatividad en Intercept, chest.depth, waist.girth, hip.girth, thigh.girth, etc.
#El p_value es muy pequeño.
residuos1=rstandard(lm1)
valores.ajustados1=fitted(lm1)
plot(valores.ajustados1, residuos1, xlab = "Valores ajustados",
ylab = "Residuos estandarizados", col = "blue")
qqplot(residuos1,valores.ajustados1)
#Normalidad de los residuos:
x=seq(-4,4,0.01)
hist(residuos1, breaks = "FD", col = "lightblue", main="Histograma de Residuos",
xlab="Residuos", prob = TRUE, ylim = c(0,0.5))
theo.res=dnorm(x, mean = 0, sd = 1)
lines(theo.res ~ x, col="violet", lwd = 2)
lines(density(residuos1), col = "blue", lwd = 2)
#Los residuos presentan una distribución aparentemente normal
qqnorm(residuos1)
qqline(residuos1, col="violet", lwd = 2)
shapiro.test(residuos1)
data<-load('acath.sav')
data
load?
data
?load
data = load('acath.sav')
data
data<-load(load='acath.sav')
data<-load(file='acath.sav')
data
summary(data)
data<-load(file='acath.sav')
data
View(acath)
acath
load(file='acath.sav')
acath
summary(acath)
dim(acath)
head(acath)
library('forecast')
source("~/.active-rstudio-document", echo=TRUE)
library('forecast')
install.packages('forecast')
library('forecast')
#Calculamos el Error Cuadrático Medio para los dos conjuntos (entrenamiento y testeo)
?auto.arima
knitr::opts_chunk$set(echo = TRUE)
library('forecast')
gas = scan('Gas6677.dat')
gas = scan('Gas6677.dat')
gas = scan(file='Gas6677.dat')
gas = scan(file='Gas6677.dat')
#Temperaturas
nottem=data("nottem")
dim(nottem)
dim(nottem)
#Temperaturas
nottem=data(nottem)
dim(nottem)
#Temperaturas
nottem
dim(nottem)
#Temperaturas
data = data("nottem")
dim(data)
#Temperaturas
data<-data("nottem")
dim(data)
#Temperaturas
data = ts(nottem, start=c(1920,1))
dim(data)
summary(data)
head(data)
data
#Temperaturas
data = ts(nottem, start=c(1920,1), frecuency=12)
#Temperaturas
data = ts(nottem, start=c(1920,1), frecuency=12)
#Temperaturas
data = ts(nottem, start=c(1920,1), frequency = 12)
summary(data)
head(data)
data
plot(data, type="p", col = "magenta", main= "Temperatura en funcion del tiempo")
lines(data, type= "l", col = "green" )
start(data, c(1920,1))
end(data, c(1939,12))
summary(data)
head(data)
data
start(data, c(1920,1))
length(data)
data
data[1]
data[length(data)]
start(data)
end(data)
boxplot(data ~ cycle(data), col = "pink")
data.desc = decompose(data)
plot(data.desc,  xlab='Año')
plot(data.desc$trend,  xlab='Año')
plot(data.desc$trend,  xlab='Año', ylab='Temperatura')
abline(reg=lm(data.desc$trend~time(data), col='blue'))
abline(reg=lm(data.desc$trend~time(data)), col = 'blue')
library(forecast)
modelo<-auto.arima(data)
summary(modelo)
train<- ts(nottem, frequency = 12, start = c(1920,1), end = c(1938,12))#le saco los últimos dos años.
train
modelo<-auto.arima(train)
modelo
modelo<-auto.arima(train, stationary = FALSE, seasonal = TRUE)
modelo
modelo
summary(modelo)
ajuste<- forecast(modelo, h = 12)
ajuste
ajuste$mean #valores ajustados
plot(ajuste)
plot(ajuste, main="Predicción de los últimos 12 meses")
lastYear<-window(data, start = C(1939,1), end=c(1939,12))
lastYear<-window(nottem, start = C(1939,1), end=c(1939,12))
lastYear<-window(data, start = C(1939,1), end=c(1939,12))
#Temperaturas
data = ts(nottem, start=c(1920,1), frequency = 12)
lastYear<-window(data, start = C(1939,1), end=c(1939,12))
SF<-window(data, start = C(1939,1), end=c(1939,12))
SF<-window(data, start = C(1939,1), end=c(1939,12))
test<- ts(nottem, frequency = 12, start = c(1939,1), end = c(1939,12))#selecciono los ultimos 12 meses.
last
test
TestVsAjuste = cbind(test, ajuste$mean)
TestVsAjuste
(1/12) * sum((ajuste$mean - test) ^ 2)
dim(acath)
load(file='acath.sav')
acath
summary(acath)
boxplot(acath$sigdz)
summary(acath$sigdz)
acath$sigdz<-factor(acath$sigdz)
acath
summary(acath)
#Regresion logística simple
logit<- glm(sigdz ~ choleste, data = acath, family = "binomial")
summary(logit)
predicto <- predict(logit, newdata, type = 'response')
#Predicción para colesterol 199
newdata = data.frame(choleste=199)
predicto <- predict(logit, newdata, type = 'response')
source("~/Especialización en Ciencias de Datos/Fundamentos de Ciencias de Datos/TP4/TP4 - CASAL - MARTINEZ REUMANN.R", echo=TRUE)
predicto
predicto2 <- predict.glm(logit, newdata, type = 'response')
predicto2
predicto <- predict(logit, newdata, type = 'response')
predicto
predicto2 <- predict.glm(logit, newdata, type = 'response')
predicto2
summary(logit)
summary(acath)
boxplot(acath$sigdz)
boxplot(acath$sex)
acath$sex<-factor(acath$sex)
acath
summary(acath)
boxplot(acath$age)
boxplot(acath)
boxplot(acath$age)
boxplot(acath$cad.dur)
boxplot(acath$choleste)
#Regresión logística con todas las variables no categóricas
logit3var<- glm(sigdz ~ choleste+age+cad.dur, data = acath, family = "binomial")
summary(logit3var)
logit3var_sex<- glm(sigdz ~ choleste+age+cad.dur+sex, data = acath, family = "binomial")
summary(logit3var_sex)
summary(logit)
summary(acath)
predicto
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ',
'devtools', 'uuid', 'digest'))
install.packages('IRkernel')
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages('IRkernel')
library( "IRkernel" )
IRkernel::installspec()
library( "IRkernel" )
IRkernel::installspec()
library( "IRkernel" )
IRkernel::installspec()
#Optimizacion Bayesiana de hiperparametros de  rpart
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rlist")
require("rpart")
require("parallel")
#paquetes necesarios para la Bayesian Optimization
require("DiceKriging")
require("mlrMBO")
#Defino la  Optimizacion Bayesiana
kBO_iter  <- 100   #cantidad de iteraciones de la Optimizacion Bayesiana
hs  <- makeParamSet(
makeNumericParam("cp"       , lower= -1   , upper=    0.1),
makeIntegerParam("minsplit" , lower=  1L  , upper= 8000L),  #la letra L al final significa ENTERO
makeIntegerParam("minbucket", lower=  1L  , upper= 2000L),
makeIntegerParam("maxdepth" , lower=  3L  , upper=   20L),
forbidden = quote( minbucket > 0.5*minsplit ) )             # minbuket NO PUEDE ser mayor que la mitad de minsplit
ksemilla_azar  <- 102199   #cambiar por la primer semilla
#------------------------------------------------------------------------------
#graba a un archivo los componentes de lista
#para el primer registro, escribe antes los titulos
loguear  <- function( reg, arch=NA, folder="./work/", ext=".txt", verbose=TRUE )
{
archivo  <- arch
if( is.na(arch) )  archivo  <- paste0( folder, substitute( reg), ext )
if( !file.exists( archivo ) )  #Escribo los titulos
{
linea  <- paste0( "fecha\t",
paste( list.names(reg), collapse="\t" ), "\n" )
cat( linea, file=archivo )
}
linea  <- paste0( format(Sys.time(), "%Y%m%d %H%M%S"),  "\t",     #la fecha y hora
gsub( ", ", "\t", toString( reg ) ),  "\n" )
cat( linea, file=archivo, append=TRUE )  #grabo al archivo
if( verbose )  cat( linea )   #imprimo por pantalla
}
#------------------------------------------------------------------------------
#particionar agrega una columna llamada fold a un dataset que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30), agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30
# particionar( data=dataset, division=c(1,1,1,1,1), agrupa=clase_ternaria, seed=semilla)   divide el dataset en 5 particiones
particionar  <- function( data, division, agrupa="", campo="fold", start=1, seed=NA )
{
if( !is.na( seed)  )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x ) }, division, seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
#------------------------------------------------------------------------------
#fold_test  tiene el numero de fold que voy a usar para testear, entreno en el resto de los folds
#param tiene los hiperparametros del arbol
ArbolSimple  <- function( fold_test, data, param )
{
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",
data= data[ fold != fold_test, ],  #entreno en todo MENOS el fold_test que uso para testing
xval= 0,
control= param )
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,
data[ fold==fold_test, ],  #aplico el modelo sobre los datos de testing
type= "prob")   #quiero que me devuelva probabilidades
prob_baja2  <- prediccion[, "BAJA+2"]  #esta es la probabilidad de baja
#calculo la ganancia
ganancia_testing  <- data[ fold==fold_test ][ prob_baja2 > 1/60,
sum( ifelse( clase_ternaria=="BAJA+2", 59000, -1000 ) )]
return( ganancia_testing )  #esta es la ganancia sobre el fold de testing, NO esta normalizada
}
#------------------------------------------------------------------------------
ArbolesCrossValidation  <- function( data, param, qfolds, pagrupa, semilla )
{
divi  <- rep( 1, qfolds )  # generalmente  c(1, 1, 1, 1, 1 )  cinco unos
particionar( data, divi, seed=semilla, agrupa=pagrupa )  #particiono en dataset en folds
ganancias  <- mcmapply( ArbolSimple,
seq(qfolds), # 1 2 3 4 5
MoreArgs= list( data, param),
SIMPLIFY= FALSE,
mc.cores= 1 )   #se puede subir a qfolds si posee Linux o Mac OS
data[ , fold := NULL ]
#devuelvo la primer ganancia y el promedio
ganancia_promedio  <- mean( unlist( ganancias ) )   #promedio las ganancias
ganancia_promedio_normalizada  <- ganancia_promedio * qfolds  #aqui normalizo la ganancia
return( ganancia_promedio_normalizada )
}
#------------------------------------------------------------------------------
#esta funcion solo puede recibir los parametros que se estan optimizando
#el resto de los parametros, lamentablemente se pasan como variables globales
EstimarGanancia  <- function( x )
{
GLOBAL_iteracion  <<-  GLOBAL_iteracion + 1
xval_folds  <- 5
ganancia  <- ArbolesCrossValidation( dataset,
param= x, #los hiperparametros del arbol
qfolds= xval_folds,  #la cantidad de folds
pagrupa= "clase_ternaria",
semilla= ksemilla_azar )
#logueo
xx  <- x
xx$xval_folds  <-  xval_folds
xx$ganancia  <- ganancia
xx$iteracion <- GLOBAL_iteracion
loguear( xx,  arch= archivo_log )
return( ganancia )
}
#------------------------------------------------------------------------------
#Aqui empieza el programa
setwd( "C:\\Users\\hmreu\\Documents\\Especialización en Ciencias de Datos\\Mineria de Datos" )
#cargo el dataset
dataset  <- fread("./datasets/paquete_premium_202011.csv")   #donde entreno
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/HT3210/", showWarnings = FALSE )
setwd("C:\\Users\\hmreu\\Documents\\Especialización en Ciencias de Datos\\Mineria de Datos\\labo\\exp\\HT3210\\")   #Establezco el Working Directory DEL EXPERIMENTO
archivo_log  <- "HT321.txt"
archivo_BO   <- "HT321.RDATA"
#leo si ya existe el log, para retomar en caso que se se corte el programa
GLOBAL_iteracion  <- 0
if( file.exists(archivo_log) )
{
tabla_log  <- fread( archivo_log )
GLOBAL_iteracion  <- nrow( tabla_log )
}
#Aqui comienza la configuracion de la Bayesian Optimization
funcion_optimizar  <- EstimarGanancia
configureMlr( show.learner.output= FALSE)
#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar
#por favor, no desesperarse por lo complejo
obj.fun  <- makeSingleObjectiveFunction(
fn=       funcion_optimizar,
minimize= FALSE,   #estoy Maximizando la ganancia
noisy=    TRUE,
par.set=  hs,
has.simple.signature = FALSE
)
ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  save.file.path= archivo_BO)
ctrl  <- setMBOControlTermination(ctrl, iters= kBO_iter )
ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())
surr.km  <- makeLearner("regr.km", predict.type= "se", covtype= "matern3_2", control= list(trace= TRUE))
#inicio la optimizacion bayesiana
if( !file.exists( archivo_BO ) ) {
run  <- mbo(obj.fun, learner = surr.km, control = ctrl)
} else  run  <- mboContinue( archivo_BO )   #retomo en caso que ya exista
require("data.table")
require("rlist")
require("rpart")
require("parallel")
#paquetes necesarios para la Bayesian Optimization
require("DiceKriging")
require("mlrMBO")
install.packages("Rcpp")
install.packages("Rcpp")
#Optimizacion Bayesiana de hiperparametros de  rpart
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rlist")
require("rpart")
require("parallel")
#paquetes necesarios para la Bayesian Optimization
require("DiceKriging")
require("mlrMBO")
#Defino la  Optimizacion Bayesiana
kBO_iter  <- 100   #cantidad de iteraciones de la Optimizacion Bayesiana
hs  <- makeParamSet(
makeNumericParam("cp"       , lower= -1   , upper=    0.1),
makeIntegerParam("minsplit" , lower=  1L  , upper= 8000L),  #la letra L al final significa ENTERO
makeIntegerParam("minbucket", lower=  1L  , upper= 2000L),
makeIntegerParam("maxdepth" , lower=  3L  , upper=   20L),
forbidden = quote( minbucket > 0.5*minsplit ) )             # minbuket NO PUEDE ser mayor que la mitad de minsplit
ksemilla_azar  <- 102199   #cambiar por la primer semilla
#------------------------------------------------------------------------------
#graba a un archivo los componentes de lista
#para el primer registro, escribe antes los titulos
loguear  <- function( reg, arch=NA, folder="./work/", ext=".txt", verbose=TRUE )
{
archivo  <- arch
if( is.na(arch) )  archivo  <- paste0( folder, substitute( reg), ext )
if( !file.exists( archivo ) )  #Escribo los titulos
{
linea  <- paste0( "fecha\t",
paste( list.names(reg), collapse="\t" ), "\n" )
cat( linea, file=archivo )
}
linea  <- paste0( format(Sys.time(), "%Y%m%d %H%M%S"),  "\t",     #la fecha y hora
gsub( ", ", "\t", toString( reg ) ),  "\n" )
cat( linea, file=archivo, append=TRUE )  #grabo al archivo
if( verbose )  cat( linea )   #imprimo por pantalla
}
#------------------------------------------------------------------------------
#particionar agrega una columna llamada fold a un dataset que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30), agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30
# particionar( data=dataset, division=c(1,1,1,1,1), agrupa=clase_ternaria, seed=semilla)   divide el dataset en 5 particiones
particionar  <- function( data, division, agrupa="", campo="fold", start=1, seed=NA )
{
if( !is.na( seed)  )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x ) }, division, seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
#------------------------------------------------------------------------------
#fold_test  tiene el numero de fold que voy a usar para testear, entreno en el resto de los folds
#param tiene los hiperparametros del arbol
ArbolSimple  <- function( fold_test, data, param )
{
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",
data= data[ fold != fold_test, ],  #entreno en todo MENOS el fold_test que uso para testing
xval= 0,
control= param )
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,
data[ fold==fold_test, ],  #aplico el modelo sobre los datos de testing
type= "prob")   #quiero que me devuelva probabilidades
prob_baja2  <- prediccion[, "BAJA+2"]  #esta es la probabilidad de baja
#calculo la ganancia
ganancia_testing  <- data[ fold==fold_test ][ prob_baja2 > 1/60,
sum( ifelse( clase_ternaria=="BAJA+2", 59000, -1000 ) )]
return( ganancia_testing )  #esta es la ganancia sobre el fold de testing, NO esta normalizada
}
#------------------------------------------------------------------------------
ArbolesCrossValidation  <- function( data, param, qfolds, pagrupa, semilla )
{
divi  <- rep( 1, qfolds )  # generalmente  c(1, 1, 1, 1, 1 )  cinco unos
particionar( data, divi, seed=semilla, agrupa=pagrupa )  #particiono en dataset en folds
ganancias  <- mcmapply( ArbolSimple,
seq(qfolds), # 1 2 3 4 5
MoreArgs= list( data, param),
SIMPLIFY= FALSE,
mc.cores= 1 )   #se puede subir a qfolds si posee Linux o Mac OS
data[ , fold := NULL ]
#devuelvo la primer ganancia y el promedio
ganancia_promedio  <- mean( unlist( ganancias ) )   #promedio las ganancias
ganancia_promedio_normalizada  <- ganancia_promedio * qfolds  #aqui normalizo la ganancia
return( ganancia_promedio_normalizada )
}
#------------------------------------------------------------------------------
#esta funcion solo puede recibir los parametros que se estan optimizando
#el resto de los parametros, lamentablemente se pasan como variables globales
EstimarGanancia  <- function( x )
{
GLOBAL_iteracion  <<-  GLOBAL_iteracion + 1
xval_folds  <- 5
ganancia  <- ArbolesCrossValidation( dataset,
param= x, #los hiperparametros del arbol
qfolds= xval_folds,  #la cantidad de folds
pagrupa= "clase_ternaria",
semilla= ksemilla_azar )
#logueo
xx  <- x
xx$xval_folds  <-  xval_folds
xx$ganancia  <- ganancia
xx$iteracion <- GLOBAL_iteracion
loguear( xx,  arch= archivo_log )
return( ganancia )
}
#------------------------------------------------------------------------------
#Aqui empieza el programa
setwd( "C:\\Users\\hmreu\\Documents\\Especialización en Ciencias de Datos\\Mineria de Datos" )
#cargo el dataset
dataset  <- fread("./datasets/paquete_premium_202011.csv")   #donde entreno
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/HT3210/", showWarnings = FALSE )
setwd("C:\\Users\\hmreu\\Documents\\Especialización en Ciencias de Datos\\Mineria de Datos\\labo\\exp\\HT3210\\")   #Establezco el Working Directory DEL EXPERIMENTO
archivo_log  <- "HT321.txt"
archivo_BO   <- "HT321.RDATA"
#leo si ya existe el log, para retomar en caso que se se corte el programa
GLOBAL_iteracion  <- 0
if( file.exists(archivo_log) )
{
tabla_log  <- fread( archivo_log )
GLOBAL_iteracion  <- nrow( tabla_log )
}
#Aqui comienza la configuracion de la Bayesian Optimization
funcion_optimizar  <- EstimarGanancia
configureMlr( show.learner.output= FALSE)
#configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar
#por favor, no desesperarse por lo complejo
obj.fun  <- makeSingleObjectiveFunction(
fn=       funcion_optimizar,
minimize= FALSE,   #estoy Maximizando la ganancia
noisy=    TRUE,
par.set=  hs,
has.simple.signature = FALSE
)
ctrl  <- makeMBOControl( save.on.disk.at.time= 600,  save.file.path= archivo_BO)
ctrl  <- setMBOControlTermination(ctrl, iters= kBO_iter )
ctrl  <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())
surr.km  <- makeLearner("regr.km", predict.type= "se", covtype= "matern3_2", control= list(trace= TRUE))
#inicio la optimizacion bayesiana
if( !file.exists( archivo_BO ) ) {
run  <- mbo(obj.fun, learner = surr.km, control = ctrl)
} else  run  <- mboContinue( archivo_BO )   #retomo en caso que ya exista
