pca = prcomp(europe[,2:8]) # Con matriz de covarianza
pca
summary(pca)
plot(pca, type='l')
biplot(pca)
pca.scale = prcomp(europe[,2:8], scale=TRUE) # Con matriz de correlacion
pca.scale
pca.scale$rotation
summary(pca.scale)
plot(pca.scale, type="l")
biplot(pca.scale)
library(ggplot2)
#install.packages(ggfortify)
library(ggfortify)
autoplot(pca.scale)
autoplot(pca.scale,label = TRUE, label.size = 3, shape = FALSE, loadings = TRUE, loadings.colour = 'blue',loadings.label = TRUE, loadings.label.size = 3)
data<-read.csv('data_simple_regression_exercice_2.csv')
data
length(data)
data<-data.frame(data)
length(data)
View(data)
View(data)
View(data)
View(data)
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
View(data)
View(data)
plot(data, xlab = "x", ylab = "y")
plot(data$x, data$y, xlab = "x", ylab = "y")
plot(data.x, data.y, xlab = "x", ylab = "y")
View(data)
View(data)
plot(data$y,data$z, xlab = "y", ylab = "z")
rectaEvsP<-lm(data$y,data$z)
rectaEvsP<-lm(data)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
rectaEvsP<-lm(data$z~data$y)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
z<-log(data$z)
plot(data$y,data$z, xlab = "y", ylab = "z")
z<-log(data$z)
rectaEvsP<-lm(z~data$y)
abline(rectaEvsP,col="red")
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
z<-log(data$z)
plot(data$y,z, xlab = "y", ylab = "z")
rectaEvsP<-lm(z~data$y)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
z<-log(data$z)
plot(data$y,z, xlab = "y", ylab = "z")
rectaEvsP<-lm(data$y~z)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
z<-log(data$z)
plot(z,data$y, xlab = "y", ylab = "z")
rectaEvsP<-lm(data$y~z)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
z<-log(data$z)
plot(z,data$y, xlab = "z", ylab = "y")
rectaEvsP<-lm(data$y~z)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
cor(data)
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
plot(data$z,data$y, xlab = "y", ylab = "z")
plot(data$z,data$y)
data<-read.csv('data_simple_regression_exercice_2.csv', sep = ' ')
data<-data.frame(data)
length(data)
plot(data$y,data$z)
rectaEvsP<-lm(data$z~data$y)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
x=log(data$z)
plot(x,data$y)
rectaEvsP<-lm(x~data$y)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
rectaEvsP<-lm(data$y~x)
abline(rectaEvsP,col="red")
summary(rectaEvsP)
cor(data)
residuos<−rstandard(rectaEvsP)
valores.ajustados<−fitted(rectaEvsP)
plot(valores.ajustados, residuos)
Cajas<-c(10, 15,10,20,25,18,12,14,16,22,24,17,13,30,24)
Distancia<-c(30,25,40,18,22,31,26,34,29,37,20,25,27,23,33)
Tiempo<-c(24,27,29,31,25,33,26,28,31,39,33,30,25,42,40)
data = data.frame('Cajas' = Cajas, 'Distancia' = Distancia, 'Tiempo' = Tiempo)
cor(data)
boxplot(data)
corplot(data)
ggcorrplot(cor(data))
source("~/Especialización en Ciencias de Datos/Fundamentos de Ciencias de Datos/TP3/Programas/EjemploCervezas.R", echo=TRUE)
library(ggcorrplot)
ggcorrplot(cor(data))
pairs(data)
rectaTD<-lm(Tiempo ~ Distancia)
summary(rectaTD)
plot(Distancia, TIempo)
plot(Distancia, Tiempo)
abline(rectaTD)
ECMTD = sum((rectaTD#residuals)^2 / length(rectaTD#residuals))
ECMTD
ECMTD = sum((rectaTD$residuals)^2 / length(rectaTD$residuals))
ECMTD
rectaTD<-lm(Tiempo ~ Cajas)
summary(rectaTD)
rectaTD<-lm(Tiempo ~ Cajas)
summary(rectaTD)
plot(Distancia, Tiempo)
abline(rectaTD)
plot(Cajas, Tiempo)
abline(rectaTD)
ECMTD = sum((rectaTD$residuals)^2 / length(rectaTD$residuals))
ECMTD
rectamultiple<-lm(Tiempo~Distancia + Cajas)
summary(rectamultiple)
summary(rectamultiple2)
rectamultiple2<-lm(Tiempo ~ Distancia + Cajas - 1 )
summary(rectamultiple2)
datos = read.csv('data_multiple_regression_course.csv')
desc(datos)
dim(datos)
summary(datos)
View(datos)
View(datos)
datos = read.csv('data_multiple_regression_course.csv', sep=".")
dim(datos)
summary(datos)
View(datos)
datos = read.csv('data_multiple_regression_course.csv', sep=" ")
dim(datos)
summary(datos)
pairs(datos)
boxplot(datos)
hist(datos$pemax)
boxplot(datos$pemax)
cor(datos)
library(ggcorrplot)
ggcorrplot(cor(datos))
# pemax Vs age
plot(datos$pemax,datos$age)
PemaxAge<-lm(datos$pemax~datos$age)
abline(PemaxAge,col="red")
# pemax Vs age
plot(datos$age,datos$pemax)
PemaxAge<-lm(datos$pemax~datos$age)
abline(PemaxAge,col="red")
summary(PemaxAge)
ggcorrplot(cor(datos))
# pemax Vs height
plot(datos$height,datos$pemax)
# pemax Vs height
plot(datos$height,datos$pemax)
PemaxHeight<-lm(datos$pemax~datos$height)
abline(PemaxHeight,col="red")
summary(PemaxHeight)
regresionMultiple<-lm(pemax ~ ., data = datos)
regresionMultiple
summary(regresionMultiple)
rstandard(rectamultiple)
residuos<-rstandard(rectamultiple)
residuos
valores.ajustados <− fitted(rectamultiple)
valores.ajustados
plot(valores.ajustados, residuos)
hist(rectamultiple$residuals,10,xlab= ̈Residuos”,main=”Distribuciones
emp ́ırica y te ́orica de los residuos”,probability=TRUE)
hist(rectamultiple$residuals,10,xlab='Residuos')
regresionMultiple<-lm(pemax ~ ., data = datos)
regresionMultiple
summary(regresionMultiple)
residuos1=rstandard(regresionMultiple)
valores.ajustados1=fitted(regresionMultiple)
plot(valores.ajustados1, residuos1, xlab = "Valores ajustados",
ylab = "Residuos estandarizados", col = "blue")
hist(residuos1)
hist(residuos1, breaks = "FD")
#Normalidad de los residuos
x=seq(-2,2,0.01)
#hist(residuos1, breaks = "FD")
hist(residuos1, breaks = "FD", col = "lightblue", main="Histograma de Residuos",
xlab="Residuos", prob = TRUE, ylim = c(0,0.4))
theo.res=dnorm(x, mean = 0, sd = 1)
lines(theo.res ~ x, col="violet", lwd = 2)
lines(density(residuos1), col = "blue", lwd = 2)
#QQPLOT
qqnorm(residuos1)
qqline(residuos1, col="violet", lwd = 2)
componentesprincipales<-prcomp(datos[,-1])
summary(componetesprincipales)
summary(componentesprincipales)
boxplot(componentesprincipales)
summary(regresionMultiple)
componentesprincipales$loadings[,1]
componentesprincipales<-prcomp(datos[,-1])
summary(componentesprincipales)
componentesprincipales$loadings
componentesprincipales$loadings
componentesprincipales
componentesprincipales$rotation
componentesprincipales$rotation[,1]
boxplot(componentesprincipales$rotation[,1])
data.body<-read.table(file="data_multiple_regression_exercice.csv")
data-body=data.frame(data.body)
attach(data.body)
#a)Dividir el dataset 80% para entrenamiento, 20% para prueba.
library(funModeling)
data.body<-read.table(file="data_multiple_regression_exercice.csv")
data-body=data.frame(data.body)
data.body=data.frame(data.body)
data.body=data.frame(data.body)
attach(data.body)
data.body<-read.table(file="data_multiple_regression_exercice.csv")
data.body=data.frame(data.body)
attach(data.body)
#a)Dividir el dataset 80% para entrenamiento, 20% para prueba.
library(funModeling)
index_sample=get_sample(data = data.body, percentage_tr_rows=0.8, seed = 1234)
data_training = data.body[index_sample,]
data_test = data.body[-index_sample,]
#b)
#Se intenta explicar el peso corporal en función del resto de las variables
#Comenzamos con un análisis de las variables
boxplot(data_training[,], col = 'violet', main = "Conjunto de Entrenamiento")
install.packages('ggcorrplot')
library(ggcorrplot)
ggcorrplot(cor(data_training[,1:24]))
#Continuamos con una regresión de peso vs todas las variables:
lm1=lm(data_training$weight~., data = data_training)
summary(lm1)
#se observa alta significatividad en Intercept, chest.depth, waist.girth, hip.girth, thigh.girth, etc.
#El p_value es muy pequeño.
residuos1=rstandard(lm1)
valores.ajustados1=fitted(lm1)
plot(valores.ajustados1, residuos1, xlab = "Valores ajustados",
ylab = "Residuos estandarizados", col = "blue")
qqplot(residuos1,valores.ajustados1)
#Normalidad de los residuos:
x=seq(-4,4,0.01)
hist(residuos1, breaks = "FD", col = "lightblue", main="Histograma de Residuos",
xlab="Residuos", prob = TRUE, ylim = c(0,0.5))
theo.res=dnorm(x, mean = 0, sd = 1)
lines(theo.res ~ x, col="violet", lwd = 2)
lines(density(residuos1), col = "blue", lwd = 2)
#Los residuos presentan una distribución aparentemente normal
qqnorm(residuos1)
qqline(residuos1, col="violet", lwd = 2)
shapiro.test(residuos1)
data<-load('acath.sav')
data
load?
data
?load
data = load('acath.sav')
data
data<-load(load='acath.sav')
data<-load(file='acath.sav')
data
summary(data)
data<-load(file='acath.sav')
data
View(acath)
acath
load(file='acath.sav')
acath
summary(acath)
dim(acath)
head(acath)
library('forecast')
source("~/.active-rstudio-document", echo=TRUE)
library('forecast')
install.packages('forecast')
library('forecast')
#Calculamos el Error Cuadrático Medio para los dos conjuntos (entrenamiento y testeo)
?auto.arima
knitr::opts_chunk$set(echo = TRUE)
library('forecast')
gas = scan('Gas6677.dat')
gas = scan('Gas6677.dat')
gas = scan(file='Gas6677.dat')
gas = scan(file='Gas6677.dat')
#Temperaturas
nottem=data("nottem")
dim(nottem)
dim(nottem)
#Temperaturas
nottem=data(nottem)
dim(nottem)
#Temperaturas
nottem
dim(nottem)
#Temperaturas
data = data("nottem")
dim(data)
#Temperaturas
data<-data("nottem")
dim(data)
#Temperaturas
data = ts(nottem, start=c(1920,1))
dim(data)
summary(data)
head(data)
data
#Temperaturas
data = ts(nottem, start=c(1920,1), frecuency=12)
#Temperaturas
data = ts(nottem, start=c(1920,1), frecuency=12)
#Temperaturas
data = ts(nottem, start=c(1920,1), frequency = 12)
summary(data)
head(data)
data
plot(data, type="p", col = "magenta", main= "Temperatura en funcion del tiempo")
lines(data, type= "l", col = "green" )
start(data, c(1920,1))
end(data, c(1939,12))
summary(data)
head(data)
data
start(data, c(1920,1))
length(data)
data
data[1]
data[length(data)]
start(data)
end(data)
boxplot(data ~ cycle(data), col = "pink")
data.desc = decompose(data)
plot(data.desc,  xlab='Año')
plot(data.desc$trend,  xlab='Año')
plot(data.desc$trend,  xlab='Año', ylab='Temperatura')
abline(reg=lm(data.desc$trend~time(data), col='blue'))
abline(reg=lm(data.desc$trend~time(data)), col = 'blue')
library(forecast)
modelo<-auto.arima(data)
summary(modelo)
train<- ts(nottem, frequency = 12, start = c(1920,1), end = c(1938,12))#le saco los últimos dos años.
train
modelo<-auto.arima(train)
modelo
modelo<-auto.arima(train, stationary = FALSE, seasonal = TRUE)
modelo
modelo
summary(modelo)
ajuste<- forecast(modelo, h = 12)
ajuste
ajuste$mean #valores ajustados
plot(ajuste)
plot(ajuste, main="Predicción de los últimos 12 meses")
lastYear<-window(data, start = C(1939,1), end=c(1939,12))
lastYear<-window(nottem, start = C(1939,1), end=c(1939,12))
lastYear<-window(data, start = C(1939,1), end=c(1939,12))
#Temperaturas
data = ts(nottem, start=c(1920,1), frequency = 12)
lastYear<-window(data, start = C(1939,1), end=c(1939,12))
SF<-window(data, start = C(1939,1), end=c(1939,12))
SF<-window(data, start = C(1939,1), end=c(1939,12))
test<- ts(nottem, frequency = 12, start = c(1939,1), end = c(1939,12))#selecciono los ultimos 12 meses.
last
test
TestVsAjuste = cbind(test, ajuste$mean)
TestVsAjuste
(1/12) * sum((ajuste$mean - test) ^ 2)
dim(acath)
load(file='acath.sav')
acath
summary(acath)
boxplot(acath$sigdz)
summary(acath$sigdz)
acath$sigdz<-factor(acath$sigdz)
acath
summary(acath)
#Regresion logística simple
logit<- glm(sigdz ~ choleste, data = acath, family = "binomial")
summary(logit)
predicto <- predict(logit, newdata, type = 'response')
#Predicción para colesterol 199
newdata = data.frame(choleste=199)
predicto <- predict(logit, newdata, type = 'response')
source("~/Especialización en Ciencias de Datos/Fundamentos de Ciencias de Datos/TP4/TP4 - CASAL - MARTINEZ REUMANN.R", echo=TRUE)
predicto
predicto2 <- predict.glm(logit, newdata, type = 'response')
predicto2
predicto <- predict(logit, newdata, type = 'response')
predicto
predicto2 <- predict.glm(logit, newdata, type = 'response')
predicto2
summary(logit)
summary(acath)
boxplot(acath$sigdz)
boxplot(acath$sex)
acath$sex<-factor(acath$sex)
acath
summary(acath)
boxplot(acath$age)
boxplot(acath)
boxplot(acath$age)
boxplot(acath$cad.dur)
boxplot(acath$choleste)
#Regresión logística con todas las variables no categóricas
logit3var<- glm(sigdz ~ choleste+age+cad.dur, data = acath, family = "binomial")
summary(logit3var)
logit3var_sex<- glm(sigdz ~ choleste+age+cad.dur+sex, data = acath, family = "binomial")
summary(logit3var_sex)
summary(logit)
summary(acath)
predicto
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon', 'pbdZMQ',
'devtools', 'uuid', 'digest'))
install.packages('IRkernel')
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages(c("repr", "IRdisplay", "evaluate", "crayon", "pbdZMQ", "devtools", "uuid", "digest"))
install.packages('IRkernel')
library( "IRkernel" )
IRkernel::installspec()
library( "IRkernel" )
IRkernel::installspec()
library( "IRkernel" )
IRkernel::installspec()
# LightGBM  Motivacional
# para motivar a los alumnos a hacer la  "Tarea Hogar DOS"
# viendo que desde el inicio de la tarea logran ganancias superadoras
# la salida queda en  "./labo/exp/KA552/KA_552_001.csv"
#los DOS puntos novedosos que se ven en este script
# 1. Se entrena  con  POS = { BAJA+1, BAJA+2 }    los BAJA+1 en realidad estan mas enfermos que los BAJA+2
#    Era forzar mucho al algoritmo agrupar los BAJAÂ¿1 con los CONTINUA
# 2. El punto anterior obliga a buscar una probabilidad de corte DISTINTA  a 1/60
# utilizar la primer semilla propia
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("lightgbm")
ksemilla  <- 102199  #poner aqui la PRIMERA de sus cinco semillas
#Aqui se debe poner la carpeta de la computadora local
setwd("C:\\Users\\hmreu\\Documents\\Especialización en Ciencias de Datos\\Mineria de Datos\\")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#paso la clase a binaria que tome valores {0,1}  enteros
#set trabaja con la clase  POS = { BAJA+1, BAJA+2 }
#esta estrategia es MUY importante
dataset[ , clase01 := ifelse( clase_ternaria %in%  c("BAJA+2","BAJA+1"), 1L, 0L) ]
#los campos que se van a utilizar
campos_buenos  <- setdiff( colnames(dataset), c("clase_ternaria","clase01") )
#dejo los datos en el formato que necesita LightGBM
dtrain  <- lgb.Dataset( data= data.matrix(  dataset[ , campos_buenos, with=FALSE]),
label= dataset$clase01 )
#genero el modelo con los parametros por default
#estos hiperparametros  salieron de una Optmizacion Bayesiana
modelo  <- lgb.train( data= dtrain,
param= list( objective=        "binary",
max_bin=              31,
learning_rate=         0.0100823763285665,
num_iterations=      588,
num_leaves=          836,
min_data_in_leaf=   2109,
feature_fraction=      0.526682065106847,
seed=               ksemilla   #aqui se utiliza SU primer semilla
)
)
#aplico el modelo a los datos sin clase
dapply  <- fread("./datasets/paquete_premium_202101.csv")
#aplico el modelo a los datos nuevos
prediccion  <- predict( modelo,
data.matrix( dapply[, campos_buenos, with=FALSE ])                                 )
#Genero la entrega para Kaggle
#Atencion ya NO corto por  1/60,  sino que busque el punto de corte optimo
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.integer(prediccion > 0.023)   ) ) #ATENCION  no es  1/60
#guardo el resultado
#creo las carpetas
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA5520/", showWarnings = FALSE )
setwd( "./labo/exp/KA5520/" )
archivo_salida  <- "KA_552_002.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep= "," )
#ahora imprimo la importancia de variables
tb_importancia  <-  as.data.table( lgb.importance(modelo) )
archivo_importancia  <- "552_importancia_002.txt"
fwrite( tb_importancia,
file= archivo_importancia,
sep= "\t" )
#cuento cuantos 1's tiene la prediccion
#cuantos estimulos estoy enviando para retener clientes
entrega[  , sum( Predicted ) ]
